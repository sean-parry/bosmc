{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432dd5e8",
   "metadata": {},
   "source": [
    "# Comparing ADAM to MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242a4c9",
   "metadata": {},
   "source": [
    "This notebook aims to illustrate the existing functionality in BoTorch for different methods of handeling the hyperparameter optimization of a gaussian process surrogate model; for bayesian optimization. For brevity discussion of different surrogate models is ommited, but both methods are generally suitable for hyperparameter optimization of multiple different surrogate models.\n",
    "\n",
    "In Bayesian Optimization for an arbitrary acquisition function we wish to find the $arg\\:max_{x \\in \\mathbb{X}} \\alpha(x \\mid \\theta)$. We can and often do choose $\\theta$ through an optimizer (e.g. ADAM). Alternatively we can consider the distribution of $\\theta$ such that we now wish to find.\n",
    "$$\n",
    "arg\\:max_{x\\in\\mathcal{X}} \\int_\\Theta \\alpha(x\\mid\\theta)P(\\theta)\\,d\\theta\n",
    "\\tag{1}\n",
    "$$\n",
    "This integral is of course intractable, a very popular way of approximating intractable integrals is monte carlo methods the most commonly used monte carlo methos is Markov Chain Monte Carlo (MCMC). \n",
    "\n",
    "The conclusion which we aim to draw in the remainder of the notebook is that the approximation of (1) through MCMC leads to better evaluations of an objective function than choosing $\\theta$ to be a single value found through an optimizer. This conclusion is one that is supported in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd0e86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We begin with defining a dataset class for readability the specifics of the class are not important, it just handels the storing of evaluations of the target (test/toy) functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852cec1c",
   "metadata": {},
   "source": [
    "## Preamble Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "321d7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tests.target_functions import BaseTarget\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(\n",
    "            self, \n",
    "            target: BaseTarget,\n",
    "        ) -> None:\n",
    "        self.target = target\n",
    "        X: torch.Tensor | None = None\n",
    "        y: torch.Tensor | None = None\n",
    "        return\n",
    "\n",
    "    def random_evals(\n",
    "            self, \n",
    "            seed: int, \n",
    "            n_iters: int\n",
    "        ) -> None:\n",
    "        assert self.X is None and self.y is None\n",
    "        random_gen = torch.Generator().manual_seed(seed)\n",
    "        rand_vals = torch.rand(size = (n_iters, self.target.dim), \n",
    "                               generator=random_gen)\n",
    "        X = self.target.bounds[0] + (self.target.bounds[1]-self.target.bounds[0]) * rand_vals\n",
    "        y = torch.tensor([self.target.sample(x) for x in X])\n",
    "        self.X = X\n",
    "        self.y = y.reshape(-1, 1)\n",
    "\n",
    "    def eval_x(\n",
    "            self,\n",
    "            x_star: torch.Tensor,\n",
    "    ) -> None:\n",
    "        y_star = self.target.sample(x_star)\n",
    "        self.X = torch.cat((self.X, x_star.unsqueeze(0)))\n",
    "        self.y = torch.cat((self.y, y_star.reshape((-1,1))))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf6b056",
   "metadata": {},
   "source": [
    "For the target functions used visit `tests > target_functions` for declarations, or see [botorch docs](https://botorch.readthedocs.io/en/latest/test_functions.html) for information on the target funcitons, the dimensions used for each will be indicated on any plots generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f0e67",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models import SingleTaskGP, SaasFullyBayesianSingleTaskGP\n",
    "\n",
    "from botorch.models.transforms import Normalize, Standardize\n",
    "from botorch.fit import fit_gpytorch_mll, fit_fully_bayesian_model_nuts, ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import LogExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from tests.target_functions import Branin\n",
    "\n",
    "import os\n",
    "\n",
    "def ADAM_BO(\n",
    "        target: BaseTarget,\n",
    "        seed: int,\n",
    "        n_random_evals: int,\n",
    "        n_bo_evals: int,\n",
    "        disable_prog_bar: bool = True,\n",
    "        save_results: bool = True\n",
    "    ) -> dict:\n",
    "    assert target.num_evals == 0, 'target must not have been evaluated'\n",
    "    \n",
    "    dataset = Dataset(target)\n",
    "    dataset.random_evals(seed, n_random_evals)\n",
    "\n",
    "    for _ in tqdm(range(n_bo_evals), \n",
    "                  disable = disable_prog_bar):\n",
    "        gp = SingleTaskGP(train_X=dataset.X,\n",
    "                          train_Y=dataset.y,\n",
    "                          input_transform=Normalize(d=target.dim),\n",
    "                          outcome_transform=Standardize(m=1),)\n",
    "        mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "        fit_gpytorch_mll(mll)\n",
    "        logEI = LogExpectedImprovement(model=gp, best_f=dataset.y.max())\n",
    "        x_star, acq_val = optimize_acqf(logEI, bounds=target.bounds, q=1, num_restarts=5, raw_samples=20)\n",
    "        x_star = x_star[0]\n",
    "        dataset.eval_x(x_star)\n",
    "\n",
    "    results = target.get_results()\n",
    "\n",
    "    if save_results:\n",
    "        save_name = f'ADAM_{target.target_name}_{seed}_{n_random_evals}_{n_bo_evals}.pkl'\n",
    "\n",
    "        with open(save_name, \"wb\") as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "    return results\n",
    "\n",
    "def MCMC_BO(\n",
    "        target: BaseTarget,\n",
    "        seed: int,\n",
    "        n_random_evals: int,\n",
    "        n_bo_evals: int,\n",
    "        warm_up_steps: int,\n",
    "        num_samples: int,\n",
    "        thinning: int,\n",
    "        disable_prog_bar: bool = True,\n",
    "        save_results: bool = True,\n",
    "    ) -> dict:\n",
    "    assert target.num_evals == 0, 'target must not have been evaluated'\n",
    "    \n",
    "    dataset = Dataset(target)\n",
    "    dataset.random_evals(seed, n_random_evals)\n",
    "\n",
    "    for _ in tqdm(range(n_bo_evals), \n",
    "                  disable = disable_prog_bar):\n",
    "        model = SaasFullyBayesianSingleTaskGP(\n",
    "            train_X=dataset.X,\n",
    "            train_Y=dataset.y,\n",
    "            input_transform=Normalize(d=target.dim),\n",
    "            outcome_transform=Standardize(m=1),)\n",
    "        fit_fully_bayesian_model_nuts(\n",
    "            model = model,\n",
    "            warmup_steps=warm_up_steps,\n",
    "            num_samples=num_samples,\n",
    "            thinning=thinning,\n",
    "            disable_progbar=True,\n",
    "        )\n",
    "        logEI = LogExpectedImprovement(model=model, best_f=dataset.y.max())\n",
    "        \n",
    "        x_star, acq_val = optimize_acqf(logEI, bounds=target.bounds, q=1, num_restarts=5, raw_samples=20)\n",
    "        x_star = x_star[0]\n",
    "        dataset.eval_x(x_star)\n",
    "\n",
    "    results = target.get_results()\n",
    "\n",
    "    if save_results:\n",
    "        save_name = f'MCMC_{target.target_name}_{seed}_{n_random_evals}_{n_bo_evals}.pkl'\n",
    "\n",
    "        with open(save_name, \"wb\") as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f81db",
   "metadata": {},
   "source": [
    "## Test Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb349e20",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (681706079.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fc79426",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
