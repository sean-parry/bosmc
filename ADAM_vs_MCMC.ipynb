{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432dd5e8",
   "metadata": {},
   "source": [
    "# Comparing ADAM to MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242a4c9",
   "metadata": {},
   "source": [
    "This notebook aims to illustrate the existing functionality in BoTorch for different methods of handeling the hyperparameter optimization of a gaussian process surrogate model; for bayesian optimization. For brevity discussion of different surrogate models is ommited, but both methods are generally suitable for hyperparameter optimization of multiple different surrogate models.\n",
    "\n",
    "In Bayesian Optimization for an arbitrary acquisition function we wish to find the $arg\\:max_{x \\in \\mathbb{X}} \\alpha(x \\mid \\theta)$. We can and often do choose $\\theta$ through an optimizer (e.g. ADAM). Alternatively we can consider the distribution of $\\theta$ such that we now wish to find.\n",
    "$$\n",
    "arg\\:max_{x\\in\\mathcal{X}} \\int_\\Theta \\alpha(x\\mid\\theta)P(\\theta)\\,d\\theta\n",
    "\\tag{1}\n",
    "$$\n",
    "This integral is of course intractable, a very popular way of approximating intractable integrals is monte carlo methods the most commonly used monte carlo methos is Markov Chain Monte Carlo (MCMC). \n",
    "\n",
    "The conclusion which we aim to draw in the remainder of the notebook is that the approximation of (1) through MCMC leads to better evaluations of an objective function than choosing $\\theta$ to be a single value found through an optimizer. This conclusion is one that is supported in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd0e86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We begin with defining a dataset class for readability the specifics of the class are not important, it just handels the storing of evaluations of the target (test/toy) functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852cec1c",
   "metadata": {},
   "source": [
    "## Preamble Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tests.target_functions import BaseTarget\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(\n",
    "            self, \n",
    "            target: BaseTarget,\n",
    "        ) -> None:\n",
    "        self.target = target\n",
    "        self.X: torch.Tensor | None = None\n",
    "        self.y: torch.Tensor | None = None\n",
    "        return\n",
    "\n",
    "    def random_evals(\n",
    "            self, \n",
    "            seed: int, \n",
    "            n_iters: int\n",
    "        ) -> None:\n",
    "        assert self.X is None and self.y is None\n",
    "        random_gen = torch.Generator().manual_seed(seed)\n",
    "        rand_vals = torch.rand(size = (n_iters, self.target.dim), \n",
    "                               generator=random_gen)\n",
    "        print(self.target.bounds)\n",
    "        X = self.target.bounds[0] + (self.target.bounds[1]-self.target.bounds[0]) * rand_vals\n",
    "        y = torch.tensor([self.target.sample(x) for x in X])\n",
    "        self.X = X\n",
    "        self.y = y.reshape(-1, 1)\n",
    "\n",
    "    def eval_x(\n",
    "            self,\n",
    "            x_star: torch.Tensor,\n",
    "    ) -> None:\n",
    "        y_star = self.target.sample(x_star)\n",
    "        self.X = torch.cat((self.X, x_star.unsqueeze(0)))\n",
    "        self.y = torch.cat((self.y, y_star.reshape((-1,1))))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf6b056",
   "metadata": {},
   "source": [
    "For the target functions used visit `tests > target_functions` for declarations, or see [botorch docs](https://botorch.readthedocs.io/en/latest/test_functions.html) for information on the target funcitons, the dimensions used for each will be indicated on any plots generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f0e67",
   "metadata": {},
   "source": [
    "## Main Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2842fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c688d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models import SingleTaskGP, SaasFullyBayesianSingleTaskGP\n",
    "\n",
    "from botorch.models.transforms import Normalize, Standardize\n",
    "from botorch.fit import fit_gpytorch_mll, fit_fully_bayesian_model_nuts, ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import LogExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "def ADAM_BO(\n",
    "        target: BaseTarget,\n",
    "        seed: int,\n",
    "        n_random_evals: int,\n",
    "        n_bo_evals: int,\n",
    "        disable_prog_bar: bool = True,\n",
    "        save_results: bool = True\n",
    "    ) -> dict:\n",
    "    assert target.num_evals == 0, 'target must not have been evaluated'\n",
    "    \n",
    "    dataset = Dataset(target)\n",
    "    dataset.random_evals(seed, n_random_evals)\n",
    "\n",
    "    for _ in tqdm(range(n_bo_evals), \n",
    "                  disable = disable_prog_bar):\n",
    "        gp = SingleTaskGP(train_X=dataset.X,\n",
    "                          train_Y=dataset.y,\n",
    "                          input_transform=Normalize(d=target.dim),\n",
    "                          outcome_transform=Standardize(m=1),)\n",
    "        gp.to(MODEL_DEVICE)\n",
    "        mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "        fit_gpytorch_mll(mll)\n",
    "        logEI = LogExpectedImprovement(model=gp, best_f=dataset.y.max())\n",
    "        x_star, acq_val = optimize_acqf(logEI, bounds=target.bounds, q=1, num_restarts=5, raw_samples=20)\n",
    "        x_star = x_star[0]\n",
    "        dataset.eval_x(x_star)\n",
    "\n",
    "    results = target.get_results()\n",
    "\n",
    "    if save_results:\n",
    "        save_name = f'data/notebook_plot_data/{target.target_name}_ADAM.pkl'\n",
    "\n",
    "        with open(save_name, \"wb\") as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "    return results\n",
    "\n",
    "def MCMC_BO(\n",
    "        target: BaseTarget,\n",
    "        seed: int,\n",
    "        n_random_evals: int,\n",
    "        n_bo_evals: int,\n",
    "        warm_up_steps: int,\n",
    "        num_samples: int,\n",
    "        thinning: int,\n",
    "        disable_prog_bar: bool = True,\n",
    "        save_results: bool = True,\n",
    "    ) -> dict:\n",
    "    assert target.num_evals == 0, 'target must not have been evaluated'\n",
    "    \n",
    "    dataset = Dataset(target)\n",
    "    dataset.random_evals(seed, n_random_evals)\n",
    "\n",
    "    for _ in tqdm(range(n_bo_evals), \n",
    "                  disable = disable_prog_bar):\n",
    "        model = SaasFullyBayesianSingleTaskGP(\n",
    "            train_X=dataset.X,\n",
    "            train_Y=dataset.y,\n",
    "            input_transform=Normalize(d=target.dim),\n",
    "            outcome_transform=Standardize(m=1),)\n",
    "        model.to(MODEL_DEVICE)\n",
    "        fit_fully_bayesian_model_nuts(\n",
    "            model = model,\n",
    "            warmup_steps=warm_up_steps,\n",
    "            num_samples=num_samples,\n",
    "            thinning=thinning,\n",
    "            disable_progbar=True,\n",
    "        )\n",
    "        logEI = LogExpectedImprovement(model=model, best_f=dataset.y.max())\n",
    "        \n",
    "        x_star, acq_val = optimize_acqf(logEI, bounds=target.bounds, q=1, num_restarts=5, raw_samples=20)\n",
    "        x_star = x_star[0]\n",
    "        dataset.eval_x(x_star)\n",
    "\n",
    "    results = target.get_results()\n",
    "\n",
    "    if save_results:\n",
    "        save_name = f'data/notebook_plot_data/{target.target_name}_MCMC.pkl'\n",
    "\n",
    "        with open(save_name, \"wb\") as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f81db",
   "metadata": {},
   "source": [
    "## Test Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb349e20",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m target: BaseTarget = target_class()\n\u001b[32m     21\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mtest_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcommon_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_args\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m end_time = time.time()\n\u001b[32m     25\u001b[39m text_line = (\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_fn.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time-start_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds to run on the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m dimensional \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.target_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m synthetic test function\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mADAM_BO\u001b[39m\u001b[34m(target, seed, n_random_evals, n_bo_evals, disable_prog_bar, save_results)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m target.num_evals == \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtarget must not have been evaluated\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     23\u001b[39m dataset = Dataset(target)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom_evals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_random_evals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_bo_evals), \n\u001b[32m     27\u001b[39m               disable = disable_prog_bar):\n\u001b[32m     28\u001b[39m     gp = SingleTaskGP(train_X=dataset.X,\n\u001b[32m     29\u001b[39m                       train_Y=dataset.y,\n\u001b[32m     30\u001b[39m                       input_transform=Normalize(d=target.dim),\n\u001b[32m     31\u001b[39m                       outcome_transform=Standardize(m=\u001b[32m1\u001b[39m),)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mDataset.random_evals\u001b[39m\u001b[34m(self, seed, n_iters)\u001b[39m\n\u001b[32m     20\u001b[39m random_gen = torch.Generator().manual_seed(seed)\n\u001b[32m     21\u001b[39m rand_vals = torch.rand(size = (n_iters, \u001b[38;5;28mself\u001b[39m.target.dim), \n\u001b[32m     22\u001b[39m                        generator=random_gen)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m X = \u001b[38;5;28mself\u001b[39m.target.bounds[\u001b[32m0\u001b[39m] + (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m-\u001b[38;5;28mself\u001b[39m.target.bounds[\u001b[32m0\u001b[39m]) * rand_vals\n\u001b[32m     24\u001b[39m y = torch.tensor([\u001b[38;5;28mself\u001b[39m.target.sample(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.X = X\n",
      "\u001b[31mIndexError\u001b[39m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tests.target_functions import Ackley, Branin, Hartmann, Rosenbrock\n",
    "\n",
    "N_RANDOM_EVALS = 5\n",
    "N_BO_EVALS = 1\n",
    "SEED = 1\n",
    "\n",
    "target_classes: list[BaseTarget] = [Ackley, Branin, Hartmann, Rosenbrock]\n",
    "test_fns: list[callable] = [ADAM_BO, MCMC_BO]\n",
    "# arguments common accross both test functions\n",
    "common_args: dict = {'n_random_evals': N_RANDOM_EVALS, 'n_bo_evals': N_BO_EVALS, 'seed': SEED}\n",
    "\n",
    "# MCMC takes extra args the number of MCMC iterations is warm_up_steps + num_samples\n",
    "# a thinning of 1 means all of num_samples are used to infer P(\\theta)\n",
    "test_args: list[dict] = [{}, {'warm_up_steps': 256, 'num_samples': 128, 'thinning': 1,}]\n",
    "\n",
    "results_text = []\n",
    "for target_class in target_classes:\n",
    "    for i, test_fn in enumerate(test_fns):\n",
    "        target: BaseTarget = target_class()\n",
    "        start_time = time.time()\n",
    "        test_fn(target = target, **common_args, **(test_args[i]))\n",
    "        end_time = time.time()\n",
    "        \n",
    "        text_line = (f'{test_fn.__name__} took {end_time-start_time} seconds to run on the {target.dim}',\n",
    "                    f' dimensional {target.target_name} synthetic test function')\n",
    "        results_text.append(text_line)\n",
    "        print(text_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc79426",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
